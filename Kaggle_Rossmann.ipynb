{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "#import h2o\n",
    "#from h2o.estimators.glm import H2OGeneralizedLinearEstimator\n",
    "#from h2o.estimators.gbm import H2OGradientBoostingEstimator\n",
    "#from h2o.estimators.random_forest import H2ORandomForestEstimator\n",
    "#from h2o.estimators.deeplearning import H2ODeepLearningEstimator\n",
    "#from h2o.h2o import _locate\n",
    "#import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.cross_validation import train_test_split\n",
    "import xgboost as xgb\n",
    "import operator\n",
    "#import matplotlib\n",
    "#import matplotlib.pyplot as plt\n",
    "\n",
    "#%matplotlib inline\n",
    "#h2o.init(max_mem_size_GB=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/python\n",
    "\n",
    "'''\n",
    "Based on https://www.kaggle.com/justdoit/rossmann-store-sales/xgboost-in-python-with-rmspe/code\n",
    "Public Score :  0.11389\n",
    "Private Validation Score :  0.096959\n",
    "'''\n",
    "def create_feature_map(features):\n",
    "    outfile = open('xgb.fmap', 'w')\n",
    "    for i, feat in enumerate(features):\n",
    "        outfile.write('{0}\\t{1}\\tq\\n'.format(i, feat))\n",
    "    outfile.close()\n",
    "\n",
    "def rmspe(y, yhat):\n",
    "    return np.sqrt(np.mean((yhat/y-0.985) ** 2))\n",
    "#0.985 & 1 \n",
    "# RMSPE2=1/2[((x−909)/909)**2+((x−1100)/1100)**2]\n",
    "# https://www.kaggle.com/c/rossmann-store-sales/forums/t/17601/correcting-log-sales-prediction-for-rmspe/99643#post99643\n",
    "def rmspe_xg(yhat, y):\n",
    "    y = np.expm1(y.get_label())\n",
    "    yhat = np.expm1(yhat)\n",
    "    return \"rmspe\", rmspe(y,yhat)\n",
    "\n",
    "# Gather some features\n",
    "def build_features(features, data):\n",
    "    # remove NaNs\n",
    "    data.fillna(0, inplace=True)\n",
    "    data.loc[data.Open.isnull(), 'Open'] = 1\n",
    "    # Use some properties directly\n",
    "    features.extend(['Store', 'CompetitionDistance', 'CompetitionOpenSinceMonth',\n",
    "                     'CompetitionOpenSinceYear', 'Promo', 'Promo2', 'Promo2SinceWeek',\n",
    "                     'Promo2SinceYear'])\n",
    "\n",
    "    # add some more with a bit of preprocessing\n",
    "    features.append('SchoolHoliday')\n",
    "    data['SchoolHoliday'] = data['SchoolHoliday'].astype(float)\n",
    "\n",
    "    features.extend(['StoreType', 'Assortment', 'StateHoliday'])\n",
    "    mappings = {'0':0, 'a':1, 'b':2, 'c':3, 'd':4}\n",
    "    data.StoreType.replace(mappings, inplace=True)\n",
    "    data.Assortment.replace(mappings, inplace=True)\n",
    "    data.StateHoliday.replace(mappings, inplace=True)\n",
    "\n",
    "    features.extend(['DayOfWeek', 'month', 'day', 'year'])\n",
    "    data['year'] = data.Date.dt.year\n",
    "    data['month'] = data.Date.dt.month\n",
    "    data['day'] = data.Date.dt.day\n",
    "    data['DayOfWeek'] = data.Date.dt.dayofweek"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start of main script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load the training, test and store data using pandas\n",
      "Assume store open, if not provided\n",
      "Consider only open stores for training. Closed stores wont count into the score.\n",
      "Use only Sales bigger then zero. Simplifies calculation of rmspe\n",
      "Join with store\n",
      "Index([u'Store', u'DayOfWeek', u'Date', u'Sales', u'Customers', u'Open',\n",
      "       u'Promo', u'StateHoliday', u'SchoolHoliday', u'logSales', u'StoreType',\n",
      "       u'Assortment', u'CompetitionDistance', u'CompetitionOpenSinceMonth',\n",
      "       u'CompetitionOpenSinceYear', u'Promo2', u'Promo2SinceWeek',\n",
      "       u'Promo2SinceYear', u'PromoInterval', u'PromoInt0', u'PromoInt1',\n",
      "       u'PromoInt2', u'PromoInt3', u'PromoInt4'],\n",
      "      dtype='object')\n",
      "augment features\n",
      "['Store', 'CompetitionDistance', 'CompetitionOpenSinceMonth', 'CompetitionOpenSinceYear', 'Promo', 'Promo2', 'Promo2SinceWeek', 'Promo2SinceYear', 'SchoolHoliday', 'StoreType', 'Assortment', 'StateHoliday', 'DayOfWeek', 'month', 'day', 'year']\n",
      "Index([u'Store', u'DayOfWeek', u'Sales', u'Open', u'Promo', u'StateHoliday',\n",
      "       u'SchoolHoliday', u'StoreType', u'Assortment', u'CompetitionDistance',\n",
      "       u'CompetitionOpenSinceMonth', u'CompetitionOpenSinceYear', u'Promo2',\n",
      "       u'Promo2SinceWeek', u'Promo2SinceYear', u'PromoInt0', u'PromoInt1',\n",
      "       u'PromoInt2', u'PromoInt3', u'PromoInt4', u'year', u'month', u'day'],\n",
      "      dtype='object')\n",
      "(872238, 23)\n",
      "Index([u'Id', u'Store', u'DayOfWeek', u'Open', u'Promo', u'StateHoliday',\n",
      "       u'SchoolHoliday', u'StoreType', u'Assortment', u'CompetitionDistance',\n",
      "       u'CompetitionOpenSinceMonth', u'CompetitionOpenSinceYear', u'Promo2',\n",
      "       u'Promo2SinceWeek', u'Promo2SinceYear', u'PromoInt0', u'PromoInt1',\n",
      "       u'PromoInt2', u'PromoInt3', u'PromoInt4', u'year', u'month', u'day'],\n",
      "      dtype='object')\n",
      "(41088, 23)\n",
      "training data processed\n"
     ]
    }
   ],
   "source": [
    "print(\"Load the training, test and store data using pandas\")\n",
    "types = {'CompetitionOpenSinceYear': np.dtype(int),\n",
    "         'CompetitionOpenSinceMonth': np.dtype(int),\n",
    "         'StateHoliday': np.dtype(str),\n",
    "         'Promo2SinceWeek': np.dtype(int),\n",
    "         'SchoolHoliday': np.dtype(float),\n",
    "         'PromoInterval': np.dtype(str)}\n",
    "train = pd.read_csv(\"/media/EA34F85134F821ED/Python27/Kaggle/Rossmann/train_filled_gap.csv\", parse_dates=[2], dtype=types)\n",
    "test = pd.read_csv(\"/media/EA34F85134F821ED/Python27/Kaggle/Rossmann/test.csv\", parse_dates=[3], dtype=types)\n",
    "store = pd.read_csv(\"/media/EA34F85134F821ED/Python27/Kaggle/Rossmann/store_prep.csv\",sep=';')\n",
    "\n",
    "print(\"Assume store open, if not provided\")\n",
    "train.fillna(1, inplace=True)\n",
    "test.fillna(1, inplace=True)\n",
    "\n",
    "print(\"Consider only open stores for training. Closed stores wont count into the score.\")\n",
    "train = train[train[\"Open\"] != 0]\n",
    "print(\"Use only Sales bigger then zero. Simplifies calculation of rmspe\")\n",
    "train = train[train[\"Sales\"] > 0]\n",
    "\n",
    "# Testinfo aus Kaggle, nicht für XGBoost da schon enthalten\n",
    "train['Sales'] = np.log1p(train['Sales'])\n",
    "\n",
    "print(\"Join with store\")\n",
    "train = train.merge(store, on='Store')\n",
    "test = test.merge(store, on='Store')\n",
    "print train.columns\n",
    "\n",
    "features = []\n",
    "\n",
    "print(\"augment features\")\n",
    "build_features(features, train)\n",
    "build_features([], test)\n",
    "print(features)\n",
    "\n",
    "\n",
    "train.drop(['PromoInterval','Date','logSales','Customers'],axis=1,inplace=True)\n",
    "train.to_csv(\"/media/EA34F85134F821ED/Python27/Kaggle/Rossmann/train__filled_gap_prep.csv\",index=False)\n",
    "             \n",
    "test.drop(['PromoInterval','Date'],axis=1,inplace=True)\n",
    "test.to_csv(\"/media/EA34F85134F821ED/Python27/Kaggle/Rossmann/test_prep.csv\",index=False)\n",
    "\n",
    "print train.columns\n",
    "print train.shape\n",
    "\n",
    "print test.columns\n",
    "print test.shape\n",
    "print('training data processed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(872238, 23)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGB Boost Train Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost Standard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"Train a XGBoost model\")\n",
    "                \n",
    "params = {\"objective\": \"reg:linear\",\n",
    "            \"booster\" : \"gbtree\",\n",
    "            \"eta\":0.858086,\n",
    "            \"max_depth\":11,\n",
    "            \"subsample\":0.963407,\n",
    "            \"colsample_bytree\":0.817634,\n",
    "            \"lambda\":0.756323,\n",
    "            \"alpha\":0.205266,\n",
    "            \"min_child_weight\":7,\n",
    "            \"num_boost_round\":9\n",
    "         }\n",
    "    \n",
    "#params = {\"objective\": \"reg:linear\",\n",
    "#          \"booster\" : \"gbtree\",\n",
    "#          \"eta\": 0.09,\n",
    "#          \"max_depth\": 10,\n",
    "#          \"subsample\": 0.9,\n",
    "#          \"colsample_bytree\": 0.7,\n",
    "#          \"silent\": 1,\n",
    "#            \"seed\": 1301\n",
    "#          }\n",
    "\n",
    "num_boost_round = 2500\n",
    "\n",
    "\n",
    "X_train, X_valid = train_test_split(train, test_size=0.012, random_state=10)\n",
    "y_train = X_train.Sales\n",
    "y_valid = X_valid.Sales\n",
    "dtrain = xgb.DMatrix(X_train[features], y_train)\n",
    "dvalid = xgb.DMatrix(X_valid[features], y_valid)\n",
    "\n",
    "watchlist = [(dtrain, 'train'), (dvalid, 'eval')]\n",
    "gbm = xgb.train(params, dtrain, num_boost_round, evals=watchlist, \\\n",
    "  early_stopping_rounds=100, feval=rmspe_xg, verbose_eval=True)\n",
    "\n",
    "print(\"Validating\")\n",
    "yhat = gbm.predict(xgb.DMatrix(X_valid[features]))\n",
    "error = rmspe(X_valid.Sales.values, np.expm1(yhat))\n",
    "print('RMSPE: {:.6f}'.format(error))\n",
    "\n",
    "print(\"Make predictions on the test set\")\n",
    "dtest = xgb.DMatrix(test[features])\n",
    "test_probs = gbm.predict(dtest)\n",
    "# Make Submission\n",
    "result = pd.DataFrame({\"Id\": test[\"Id\"], 'Sales': np.expm1(test_probs)})\n",
    "result.to_csv('xgboost_STANDARD.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost GridSearch\n",
    "XGB1 * 0.16 + XGB2*0.39 + XGB3 *0.39 + RF * 0.06"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "features = [u'Store', u'DayOfWeek', u'Open', u'Promo', u'StateHoliday',\n",
    "       u'SchoolHoliday', u'StoreType', u'Assortment', u'CompetitionDistance',\n",
    "       u'CompetitionOpenSinceMonth', u'CompetitionOpenSinceYear', u'Promo2',\n",
    "       u'Promo2SinceWeek', u'Promo2SinceYear', u'PromoInt0', u'PromoInt1',\n",
    "       u'PromoInt2', u'PromoInt3', u'PromoInt4', u'year', u'month', u'day']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Will train until eval error hasn't decreased in 100 rounds.\n",
      "[0]\ttrain-rmspe:0.984436\teval-rmspe:0.984437\n",
      "[1]\ttrain-rmspe:0.983390\teval-rmspe:0.983396\n",
      "[2]\ttrain-rmspe:0.981175\teval-rmspe:0.981197\n",
      "[3]\ttrain-rmspe:0.976886\teval-rmspe:0.976942\n",
      "[4]\ttrain-rmspe:0.969293\teval-rmspe:0.969419\n",
      "[5]\ttrain-rmspe:0.956999\teval-rmspe:0.957250\n",
      "[6]\ttrain-rmspe:0.938749\teval-rmspe:0.939198\n",
      "[7]\ttrain-rmspe:0.914191\teval-rmspe:0.914922\n",
      "[8]\ttrain-rmspe:0.884766\teval-rmspe:0.885850\n",
      "[9]\ttrain-rmspe:0.854811\teval-rmspe:0.856275\n",
      "[10]\ttrain-rmspe:0.832662\teval-rmspe:0.834439\n",
      "[11]\ttrain-rmspe:0.829966\teval-rmspe:0.831852\n",
      "[12]\ttrain-rmspe:0.859190\teval-rmspe:0.860868\n",
      "[13]\ttrain-rmspe:0.928305\teval-rmspe:0.929455\n",
      "[14]\ttrain-rmspe:1.037280\teval-rmspe:1.037690\n",
      "[15]\ttrain-rmspe:1.179229\teval-rmspe:1.178854\n",
      "[16]\ttrain-rmspe:1.343934\teval-rmspe:1.342825\n",
      "[17]\ttrain-rmspe:1.522453\teval-rmspe:1.520700\n",
      "[18]\ttrain-rmspe:1.706053\teval-rmspe:1.703779\n",
      "[19]\ttrain-rmspe:1.888708\teval-rmspe:1.886038\n",
      "[20]\ttrain-rmspe:2.064656\teval-rmspe:2.061685\n",
      "[21]\ttrain-rmspe:2.230955\teval-rmspe:2.227784\n",
      "[22]\ttrain-rmspe:2.385181\teval-rmspe:2.381866\n",
      "[23]\ttrain-rmspe:2.525736\teval-rmspe:2.522330\n",
      "[24]\ttrain-rmspe:2.652133\teval-rmspe:2.648673\n",
      "[25]\ttrain-rmspe:2.763610\teval-rmspe:2.760122\n",
      "[26]\ttrain-rmspe:2.860225\teval-rmspe:2.856748\n",
      "[27]\ttrain-rmspe:2.942978\teval-rmspe:2.939495\n",
      "[28]\ttrain-rmspe:3.012223\teval-rmspe:3.008744\n",
      "[29]\ttrain-rmspe:3.070153\teval-rmspe:3.066658\n",
      "[30]\ttrain-rmspe:3.116340\teval-rmspe:3.112816\n",
      "[31]\ttrain-rmspe:3.151442\teval-rmspe:3.147867\n",
      "[32]\ttrain-rmspe:3.179106\teval-rmspe:3.175462\n",
      "[33]\ttrain-rmspe:3.198702\teval-rmspe:3.194966\n",
      "[34]\ttrain-rmspe:3.212129\teval-rmspe:3.208274\n",
      "[35]\ttrain-rmspe:3.218256\teval-rmspe:3.214252\n",
      "[36]\ttrain-rmspe:3.220122\teval-rmspe:3.215945\n",
      "[37]\ttrain-rmspe:3.216407\teval-rmspe:3.212032\n",
      "[38]\ttrain-rmspe:3.210302\teval-rmspe:3.205705\n",
      "[39]\ttrain-rmspe:3.201284\teval-rmspe:3.196426\n",
      "[40]\ttrain-rmspe:3.188175\teval-rmspe:3.183033\n",
      "[41]\ttrain-rmspe:3.173420\teval-rmspe:3.167969\n",
      "[42]\ttrain-rmspe:3.157068\teval-rmspe:3.151275\n",
      "[43]\ttrain-rmspe:3.138646\teval-rmspe:3.132486\n",
      "[44]\ttrain-rmspe:3.120133\teval-rmspe:3.113585\n",
      "[45]\ttrain-rmspe:3.100491\teval-rmspe:3.093529\n",
      "[46]\ttrain-rmspe:3.080089\teval-rmspe:3.072680\n",
      "[47]\ttrain-rmspe:3.059319\teval-rmspe:3.051443\n",
      "[48]\ttrain-rmspe:3.038134\teval-rmspe:3.029766\n",
      "[49]\ttrain-rmspe:3.017103\teval-rmspe:3.008219\n",
      "[50]\ttrain-rmspe:2.995280\teval-rmspe:2.985863\n",
      "[51]\ttrain-rmspe:2.971738\teval-rmspe:2.961770\n",
      "[52]\ttrain-rmspe:2.950832\teval-rmspe:2.940288\n",
      "[53]\ttrain-rmspe:2.930207\teval-rmspe:2.919070\n",
      "[54]\ttrain-rmspe:2.909790\teval-rmspe:2.898037\n",
      "[55]\ttrain-rmspe:2.889764\teval-rmspe:2.877379\n",
      "[56]\ttrain-rmspe:2.870103\teval-rmspe:2.857069\n",
      "[57]\ttrain-rmspe:2.850833\teval-rmspe:2.837138\n",
      "[58]\ttrain-rmspe:2.831924\teval-rmspe:2.817554\n",
      "[59]\ttrain-rmspe:2.813291\teval-rmspe:2.798235\n",
      "[60]\ttrain-rmspe:2.794349\teval-rmspe:2.778597\n",
      "[61]\ttrain-rmspe:2.776690\teval-rmspe:2.760230\n",
      "[62]\ttrain-rmspe:2.759629\teval-rmspe:2.742450\n",
      "[63]\ttrain-rmspe:2.742548\teval-rmspe:2.724644\n",
      "[64]\ttrain-rmspe:2.726164\teval-rmspe:2.707528\n",
      "[65]\ttrain-rmspe:2.710337\teval-rmspe:2.690963\n",
      "[66]\ttrain-rmspe:2.694846\teval-rmspe:2.674731\n",
      "[67]\ttrain-rmspe:2.679747\teval-rmspe:2.658884\n",
      "[68]\ttrain-rmspe:2.665166\teval-rmspe:2.643553\n",
      "[69]\ttrain-rmspe:2.651056\teval-rmspe:2.628693\n",
      "[70]\ttrain-rmspe:2.636983\teval-rmspe:2.613870\n",
      "[71]\ttrain-rmspe:2.623570\teval-rmspe:2.599708\n",
      "[72]\ttrain-rmspe:2.610692\teval-rmspe:2.586080\n",
      "[73]\ttrain-rmspe:2.598173\teval-rmspe:2.572816\n",
      "[74]\ttrain-rmspe:2.585805\teval-rmspe:2.559704\n",
      "[75]\ttrain-rmspe:2.574096\teval-rmspe:2.547253\n",
      "[76]\ttrain-rmspe:2.562513\teval-rmspe:2.534934\n",
      "[77]\ttrain-rmspe:2.551096\teval-rmspe:2.522789\n",
      "[78]\ttrain-rmspe:2.539988\teval-rmspe:2.510958\n",
      "[79]\ttrain-rmspe:2.529539\teval-rmspe:2.499793\n",
      "[80]\ttrain-rmspe:2.519431\teval-rmspe:2.488973\n",
      "[81]\ttrain-rmspe:2.509678\teval-rmspe:2.478519\n",
      "[82]\ttrain-rmspe:2.499779\teval-rmspe:2.467928\n",
      "[83]\ttrain-rmspe:2.490174\teval-rmspe:2.457639\n",
      "[84]\ttrain-rmspe:2.480940\teval-rmspe:2.447731\n",
      "[85]\ttrain-rmspe:2.471637\teval-rmspe:2.437765\n",
      "[86]\ttrain-rmspe:2.462897\teval-rmspe:2.428371\n",
      "[87]\ttrain-rmspe:2.454060\teval-rmspe:2.418892\n",
      "[88]\ttrain-rmspe:2.445742\teval-rmspe:2.409941\n",
      "[89]\ttrain-rmspe:2.437414\teval-rmspe:2.400993\n",
      "[90]\ttrain-rmspe:2.429370\teval-rmspe:2.392341\n",
      "[91]\ttrain-rmspe:2.421764\teval-rmspe:2.384136\n",
      "[92]\ttrain-rmspe:2.414260\teval-rmspe:2.376046\n",
      "[93]\ttrain-rmspe:2.406760\teval-rmspe:2.367972\n",
      "[94]\ttrain-rmspe:2.399560\teval-rmspe:2.360214\n",
      "[95]\ttrain-rmspe:2.392595\teval-rmspe:2.352700\n",
      "[96]\ttrain-rmspe:2.385758\teval-rmspe:2.345326\n",
      "[97]\ttrain-rmspe:2.379056\teval-rmspe:2.338101\n",
      "[98]\ttrain-rmspe:2.372362\teval-rmspe:2.330898\n",
      "[99]\ttrain-rmspe:2.365614\teval-rmspe:2.323653\n",
      "[100]\ttrain-rmspe:2.359227\teval-rmspe:2.316784\n",
      "[101]\ttrain-rmspe:2.352784\teval-rmspe:2.309870\n",
      "[102]\ttrain-rmspe:2.346549\teval-rmspe:2.303176\n",
      "[103]\ttrain-rmspe:2.340310\teval-rmspe:2.296492\n",
      "[104]\ttrain-rmspe:2.334180\teval-rmspe:2.289931\n",
      "[105]\ttrain-rmspe:2.328123\teval-rmspe:2.283453\n",
      "[106]\ttrain-rmspe:2.322261\teval-rmspe:2.277184\n",
      "[107]\ttrain-rmspe:2.316667\teval-rmspe:2.271192\n",
      "[108]\ttrain-rmspe:2.311056\teval-rmspe:2.265195\n",
      "[109]\ttrain-rmspe:2.305426\teval-rmspe:2.259193\n",
      "[110]\ttrain-rmspe:2.299549\teval-rmspe:2.252959\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Train a XGBoost model', 0.009, 130, 0.5, 0.5)\n",
      "Validating\n",
      "RMSPE: 1795.732371\n",
      "Make predictions on the test set\n",
      "Delete....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[111]\ttrain-rmspe:2.294072\teval-rmspe:2.247134\n",
      "Stopping. Best iteration:\n",
      "[11]\ttrain-rmspe:0.829966\teval-rmspe:0.831852\n",
      "\n"
     ]
    }
   ],
   "source": [
    "num_boost_round = 2500\n",
    "\n",
    "\n",
    "X_train, X_valid = train_test_split(train, test_size=0.012, random_state=10)\n",
    "\n",
    "y_train = X_train.Sales\n",
    "y_valid = X_valid.Sales\n",
    "\n",
    "#features = train.columns\n",
    "#features.remove('Sales')\n",
    "\n",
    "dtrain = xgb.DMatrix(X_train[features], y_train)\n",
    "dvalid = xgb.DMatrix(X_valid[features], y_valid)\n",
    "\n",
    "\n",
    "for eta in [0.009]:\n",
    "    for max_depth in [130]:\n",
    "        for subsample in [0.5]:\n",
    "            for colsample_bytree in [0.5]:\n",
    "                print(\"Train a XGBoost model\", eta, max_depth, subsample, colsample_bytree)\n",
    "\n",
    "\n",
    "#    \"reg:linear\" --linear regression\n",
    "#    \"reg:logistic\" --logistic regression\n",
    "#    \"binary:logistic\" --logistic regression for binary classification, output probability\n",
    "#   \"binary:logitraw\" --logistic regression for binary classification, output score before logistic transformation\n",
    "#    \"count:poisson\" --poisson regression for count data, output mean of poisson distribution\n",
    "#        max_delta_step is set to 0.7 by default in poisson regression (used to safeguard optimization)\n",
    "#    \"multi:softmax\" --set XGBoost to do multiclass classification using the softmax objective, you also need to set num_class(number of classes)\n",
    "#    \"multi:softprob\" --same as softmax, but output a vector of ndata * nclass, which can be further reshaped to ndata, nclass matrix. The result contains predicted probability of each data point belonging to each class.\n",
    "#   \"rank:pairwise\" --set XGBoost to do ranking task by minimizing the pairwise loss\n",
    "\n",
    "                params = {\"objective\": \"reg:linear\",\n",
    "                          \"booster\" : \"gblinear\", #getree, gblinear\n",
    "                          \"eta\": eta,\n",
    "                          \"max_depth\": max_depth,\n",
    "                          \"subsample\": subsample,\n",
    "                          \"colsample_bytree\": colsample_bytree,\n",
    "                          \"silent\": 1,\n",
    "                              \"seed\": 1301\n",
    "                          }\n",
    "\n",
    "\n",
    "                watchlist = [(dtrain, 'train'), (dvalid, 'eval')]\n",
    "                gbm = xgb.train(params, dtrain, num_boost_round, evals=watchlist, \\\n",
    "                  early_stopping_rounds=100, feval=rmspe_xg, verbose_eval=True)\n",
    "\n",
    "                print(\"Validating\")\n",
    "                yhat = gbm.predict(xgb.DMatrix(X_valid[features]))\n",
    "                error = rmspe(X_valid.Sales.values, np.expm1(yhat))\n",
    "                print('RMSPE: {:.6f}'.format(error))\n",
    "\n",
    "                print(\"Make predictions on the test set\")\n",
    "                dtest = xgb.DMatrix(test[features])\n",
    "                test_probs = gbm.predict(dtest)\n",
    "                # Make Submission\n",
    "                result = pd.DataFrame({\"Id\": test[\"Id\"], 'Sales': np.expm1(test_probs)})\n",
    "                result.to_csv('xgboost_gblinear_RMSPE985_'+str(error)+'_'+str(eta)+'_'+str(max_depth)+'_'+str(subsample)+'_'+str(colsample_bytree)+'.csv', index=False)\n",
    "            del gbm, result, test_probs, dtest, yhat, error\n",
    "            print 'Delete....'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Blend XGBoost\n",
    "XGB1 * 0.16 + XGB2*0.39 + XGB3 *0.39 + RF * 0.06"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "eins = pd.read_csv('/media/EA34F85134F821ED/Python27/Kaggle/Rossmann/xgboost_0.0864089444975_0.009_15_0.7_0.7.csv')\n",
    "zwei = pd.read_csv('/media/EA34F85134F821ED/Python27/Kaggle/Rossmann/xgboost_0.0868335094218_0.009_15_0.5_0.7.csv')\n",
    "drei = pd.read_csv('/media/EA34F85134F821ED/Python27/Kaggle/Rossmann/xgboost_0.0868709590517_0.009_15_0.7_1.csv')\n",
    "vier = pd.read_csv('/media/EA34F85134F821ED/Python27/Kaggle/Rossmann/xgboost_0.0868830056844_0.009_15_0.5_1.csv')\n",
    "fuenf = pd.read_csv('/media/EA34F85134F821ED/Python27/Kaggle/Rossmann/xgboost_0.0884385627912_0.009_15_0.5_0.5.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "blend = pd.DataFrame()\n",
    "blend['Id'] = eins.Id\n",
    "blend['Sales'] = 0.10*eins.Sales + 0.10*zwei.Sales + 0.10*drei.Sales + 0.10*vier.Sales + 0.60*fuenf.Sales\n",
    "blend.to_csv('blend_xgboost.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGB feature importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'gbm' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-bd59a33486bf>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mcreate_feature_map\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mimportance\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgbm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_fscore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfmap\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'xgb.fmap'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mimportance\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimportance\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moperator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitemgetter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'gbm' is not defined"
     ]
    }
   ],
   "source": [
    "# XGB feature importances\n",
    "\n",
    "create_feature_map(features)\n",
    "importance = gbm.get_fscore(fmap='xgb.fmap')\n",
    "importance = sorted(importance.items(), key=operator.itemgetter(1))\n",
    "\n",
    "df = pd.DataFrame(importance, columns=['feature', 'fscore'])\n",
    "df['fscore'] = df['fscore'] / df['fscore'].sum()\n",
    "\n",
    "featp = df.plot(kind='barh', x='feature', y='fscore', legend=False, figsize=(6, 10))\n",
    "plt.title('XGBoost Feature Importance')\n",
    "plt.xlabel('relative importance')\n",
    "fig_featp = featp.get_figure()\n",
    "#fig_featp.savefig('feature_importance_xgb.png', bbox_inches='tight', pad_inches=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Populating XGboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_valid = train_test_split(train, test_size=0.012)\n",
    "y_train = X_train.Sales\n",
    "y_valid = X_valid.Sales\n",
    "dtrain = xgb.DMatrix(X_train[features], y_train)\n",
    "dvalid = xgb.DMatrix(X_valid[features], y_valid)\n",
    "\n",
    "\n",
    "##############################Evolutionary search #########################\n",
    "'''\n",
    "Data preprocessing is done. Now we are in place for some parameter optimization. \n",
    "Evolutionary Algorithm is a randomized meta optimization procedure that mimics natural evolution.\n",
    "The algorithm proceeds with first creating an initial random population of\n",
    "parameter values. The instances are scored using xgboost 5-fold CV. \n",
    "Next, a new generation of population  is created as follows:\n",
    "- A small proportion of elite (i.e. top scoring) individuals is carried forward directly to the new population\n",
    "- The rest of the population is filled with randomly created individuals, by:\n",
    "    +randomly picking two parents from the top performing individuals of the last population (e.g. top 50%)\n",
    "    +combine the 'genes' (parameter values) randomly to create a new individual that inherits \n",
    "    50% of the genes from each parent.\n",
    "    +with a small probability, we mutate some gene's value\n",
    "    \n",
    "- The new population is evaluated, and the loop continues until convergence, or until \n",
    "a predefined number of generations has been reached. \n",
    "'''\n",
    "\n",
    "from random import randint\n",
    "import random\n",
    "\n",
    "popSize=5; #population size, set from 20 to 100\n",
    "eliteSize=0.1; #percentage of elite instances to be ratained \n",
    "\n",
    "paramList=['depth','nRound','eta','gamma','min_child_weight','lamda','alpha','colsample_bytree','subsample','fitness']\n",
    "\n",
    "#Creating an initial population\n",
    "population=pd.DataFrame(np.zeros(shape=(popSize,len(paramList))),columns = paramList);\n",
    "population.depth=[randint(6,15) for p in range(0,popSize)]\n",
    "#population.nRound=[randint(50,500) for p in range(0,popSize)]  #number of boosting round\n",
    "population.nRound=[randint(5,10) for p in range(0,popSize)] #quick test\n",
    "population.eta=[random.uniform(0.6, 1) for p in range(0,popSize)]\n",
    "population.gamma=[random.uniform(0.01, 0.03) for p in range(0,popSize)]\n",
    "population.min_child_weight=[randint(1,20) for p in range(0,popSize)]\n",
    "population.lamda =[random.uniform(0.1,1) for p in range(0,popSize)]\n",
    "population.alpha =[random.uniform(0.1, 1) for p in range(0,popSize)]\n",
    "population.colsample_bytree=[random.uniform(0.7, 1) for p in range(0,popSize)]\n",
    "population.subsample=[random.uniform(0.7, 1) for p in range(0,popSize)]\n",
    "population.fitness=[random.uniform(100, 100) for p in range(0,popSize)]\n",
    "\n",
    "#Creating a new population based on an existing one\n",
    "def createNewPopulation(population,eliteSize=0.1,mutation_rate=0.2):\n",
    "    population.sort(['fitness'],ascending=1,inplace=True)\n",
    "    population.reset_index(drop=True,inplace=True)\n",
    "    popSize=population.shape[0]\n",
    "    nElite=int(round(eliteSize*popSize))\n",
    "    \n",
    "    new_population=population.copy(deep=True);\n",
    "    for i in range(nElite,popSize):    #form a new population from the top 50% instances\n",
    "        #get two random parents\n",
    "        p1=randint(nElite,int(popSize/2))\n",
    "        p2=randint(nElite,int(popSize/2))\n",
    "        \n",
    "        for attr in list(new_population.columns.values):\n",
    "            #print attr, population[attr][i]\n",
    "            if(random.uniform(0,1)>0.5 ):\n",
    "                new_population.ix[i,attr]=population.ix[p1,attr]\n",
    "            else:\n",
    "                new_population.ix[i,attr]=population.ix[p2,attr]\n",
    "\n",
    "            #injecting some mutation\n",
    "            if(random.uniform(0,1)<mutation_rate ):\n",
    "                attr=list(new_population.columns.values)[randint(0,8)]\n",
    "                if(attr=='depth'):\n",
    "                    new_population.ix[i,attr]= max(3,new_population.ix[i,attr]+randint(-2,2))\n",
    "                elif(attr=='nRound'):\n",
    "                    new_population.ix[i,attr]= max(10,new_population.ix[i,attr]+randint(-50,50))\n",
    "                elif(attr=='eta'):\n",
    "                    new_population.ix[i,attr]= max(0.1,new_population.ix[i,attr]+random.uniform(-0.05,0.05))\n",
    "                elif(attr=='gamma'):\n",
    "                    new_population.ix[i,attr]= max(0.1,new_population.ix[i,attr]+random.uniform(-0.005,0.005))\n",
    "                elif(attr=='min_child_weight'):\n",
    "                    new_population.ix[i,attr]= max(0,new_population.ix[i,attr]+randint(-2,2)  )                  \n",
    "                elif(attr=='lamda'):\n",
    "                    new_population.ix[i,attr]= max(0.1,new_population.ix[i,attr]+random.uniform(-0.05,0.05))                   \n",
    "                elif(attr=='alpha'):\n",
    "                    new_population.ix[i,attr]= max(0.1,new_population.ix[i,attr]+random.uniform(-0.05,0.05))                   \n",
    "                elif(attr=='colsample_bytree'):\n",
    "                    new_population.ix[i,attr]= max(0.6,new_population.ix[i,attr]+random.uniform(-0.05,0.05)) \n",
    "                elif(attr=='subsample'):\n",
    "                    new_population.ix[i,attr]= max(0.6,new_population.ix[i,attr]+random.uniform(-0.05,0.05))                      \n",
    "    return new_population\n",
    "\n",
    "#score each instance using 5-fold CV\n",
    "def testInstance(population,i,dtrain):\n",
    "    params = {\"objective\": \"reg:linear\",\n",
    "          \"eta\": population.eta[i],\n",
    "          \"max_depth\": population.depth[i],\n",
    "          \"subsample\": population.subsample[i],\n",
    "          \"colsample_bytree\": population.colsample_bytree[i],\n",
    "          \"num_boost_round\":int(population.nRound[i]),\n",
    "          \"lambda\":population.lamda[i],\n",
    "          \"alpha\":population.alpha[i],\n",
    "          \"gamma\":population.gamma[i],\n",
    "          \"min_child_weight\":population.min_child_weight[i],\n",
    "          \"silent\": 1,\n",
    "          #\"seed\": 1301\n",
    "          } \n",
    "    history = xgb.cv(\n",
    "        params,\n",
    "        dtrain,  \n",
    "        #early_stopping_rounds=30, #no early stopping in Python yet!!!\n",
    "        num_boost_round  =int(population.nRound[i]),\n",
    "        nfold=5, # number of CV folds\n",
    "        nthread=-1, # number of CPU threads  \n",
    "        show_progress=False,\n",
    "        feval=rmspe_xg, # custom evaluation metric\n",
    "        obj=RMSPE_objective\n",
    "        #maximize=0 # the lower the evaluation score the better\n",
    "        )\n",
    "    return history[\"test-rmspe-mean\"].iget(-1)\n",
    "\n",
    "def printResult(filename,population,i,generation): #print best instances to file\n",
    "    f1=open(filename, 'a')\n",
    "    f1.write('Generation %d Best fitness %f\\n' % (generation , population.fitness[i]))\n",
    "    f1.write('\"eta\":%f\\n'%population.eta[i])    \n",
    "    f1.write('\"max_depth\":%f\\n'%population.depth[i])    \n",
    "    f1.write('\"subsample\":%f\\n'%population.subsample[i])    \n",
    "    f1.write('\"colsample_bytree\":%f\\n'%population.colsample_bytree[i])    \n",
    "    f1.write('\"lambda\":%f\\n'%population.lamda[i])    \n",
    "    f1.write('\"alpha\":%f\\n'%population.alpha[i])    \n",
    "    f1.write('\"min_child_weight\":%f\\n'%population.min_child_weight[i])    \n",
    "    f1.write('\"num_boost_round\":%f\\n'%population.nRound[i])  \n",
    "    f1.close()\n",
    "           \n",
    "#Main loop of the Evolutionary Algorithm: \n",
    "#Populations are created and avaluated.\n",
    "\n",
    "nGeneration=2; #number of generations\n",
    "for run in range(nGeneration):\n",
    "    print(\"Generation %d\\n\" %run)\n",
    "    population=createNewPopulation(population,eliteSize=0.1,mutation_rate=0.2)\n",
    "    for i in range(popSize):\n",
    "        print (\"Testing instance %d \"%i)\n",
    "        population.ix[i,'fitness']=testInstance(population,i,dtrain)\n",
    "        print (\"--Fitness %f \\n \" % population.fitness[i])\n",
    "    population.sort(['fitness'],ascending=1,inplace=True)\n",
    "    population.reset_index(drop=True,inplace=True)\n",
    "    printResult('Result.txt',population,0,run) #print best instances to file\n",
    "    print(\"Generation %d Best fitness (5-fold RMSPE CV): %f\" %(run, population.fitness[0]))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##############################Testing#########################\n",
    "i=0 #selecting the best instance\n",
    "params = {\"objective\": \"reg:linear\",\n",
    "          \"eta\": population.eta[i],\n",
    "          \"max_depth\": population.depth[i],\n",
    "          \"subsample\": population.subsample[i],\n",
    "          \"colsample_bytree\": population.colsample_bytree[i],\n",
    "          \"num_boost_round\":int(population.nRound[i]),\n",
    "          \"lambda\":population.lamda[i],\n",
    "          \"alpha\":population.alpha[i],\n",
    "          \"gamma\":population.gamma[i],\n",
    "          \"min_child_weight\":population.min_child_weight[i],\n",
    "          \"silent\": 1          \n",
    "} \n",
    "#train the final xgboost model\n",
    "gbm=xgb.train(params, dtrain,  feval=rmspe_xg, num_boost_round=int(population.nRound[i]), obj=RMSPE_objective, verbose_eval=True)\n",
    "          \n",
    "print(\"Make predictions on the test set\")\n",
    "dtest = xgb.DMatrix(test[features])\n",
    "test_probs = gbm.predict(dtest)\n",
    "# Make Submission\n",
    "result = pd.DataFrame({\"Id\": test[\"Id\"], 'Sales': np.expm1(test_probs)})\n",
    "result.to_csv(\"xgboost_10_submission.csv\", index=False)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost + Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/python\n",
    "#from __future__ import print_function\n",
    "'''\n",
    "Public Score :  0.11771\n",
    "Private Validation Score : [3886] train-rmspe:0.062407  eval-rmspe:0.088543\n",
    "'''\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.cross_validation import train_test_split\n",
    "import xgboost as xgb\n",
    "\n",
    "# Gather some features\n",
    "def build_features(features, data):\n",
    "    # remove NaNs\n",
    "    data.fillna(0, inplace=True)\n",
    "    data.loc[data.Open.isnull(), 'Open'] = 1\n",
    "    # Use some properties directly\n",
    "    features.extend(['Store', 'CompetitionDistance', 'Promo', 'Promo2', 'SchoolHoliday'])\n",
    "\n",
    "    # Label encode some features\n",
    "    features.extend(['StoreType', 'Assortment', 'StateHoliday'])\n",
    "    mappings = {'0':0, 'a':1, 'b':2, 'c':3, 'd':4}\n",
    "    data.StoreType.replace(mappings, inplace=True)\n",
    "    data.Assortment.replace(mappings, inplace=True)\n",
    "    data.StateHoliday.replace(mappings, inplace=True)\n",
    "\n",
    "    features.extend(['DayOfWeek', 'Month', 'Day', 'Year', 'WeekOfYear'])\n",
    "    data['Year'] = data.Date.dt.year\n",
    "    data['Month'] = data.Date.dt.month\n",
    "    data['Day'] = data.Date.dt.day\n",
    "    data['DayOfWeek'] = data.Date.dt.dayofweek\n",
    "    data['WeekOfYear'] = data.Date.dt.weekofyear\n",
    "\n",
    "    # CompetionOpen en PromoOpen from https://www.kaggle.com/ananya77041/rossmann-store-sales/randomforestpython/code\n",
    "    # Calculate time competition open time in months\n",
    "    features.append('CompetitionOpen')\n",
    "    data['CompetitionOpen'] = 12 * (data.Year - data.CompetitionOpenSinceYear) + \\\n",
    "        (data.Month - data.CompetitionOpenSinceMonth)\n",
    "    # Promo open time in months\n",
    "    features.append('PromoOpen')\n",
    "    data['PromoOpen'] = 12 * (data.Year - data.Promo2SinceYear) + \\\n",
    "        (data.WeekOfYear - data.Promo2SinceWeek) / 4.0\n",
    "    data['PromoOpen'] = data.PromoOpen.apply(lambda x: x if x > 0 else 0)\n",
    "\n",
    "    # Indicate that sales on that day are in promo interval\n",
    "    features.append('IsPromoMonth')\n",
    "    month2str = {1:'Jan', 2:'Feb', 3:'Mar', 4:'Apr', 5:'May', 6:'Jun', \\\n",
    "             7:'Jul', 8:'Aug', 9:'Sept', 10:'Okt', 11:'Nov', 12:'Dec'}\n",
    "    data['monthStr'] = data.Month.map(month2str)\n",
    "    data.loc[data.PromoInterval == 0, 'PromoInterval'] = ''\n",
    "    data['IsPromoMonth'] = 0\n",
    "    for interval in data.PromoInterval.unique():\n",
    "        if interval != '':\n",
    "            for month in interval.split(','):\n",
    "                data.loc[(data.monthStr == month) & (data.PromoInterval == interval), 'IsPromoMonth'] = 1\n",
    "\n",
    "    return data\n",
    "\n",
    "## Start of main script\n",
    "\n",
    "print(\"Load the training, test and store data using pandas\")\n",
    "types = {'CompetitionOpenSinceYear': np.dtype(int),\n",
    "         'CompetitionOpenSinceMonth': np.dtype(int),\n",
    "         'StateHoliday': np.dtype(str),\n",
    "         'Promo2SinceWeek': np.dtype(int),\n",
    "         'SchoolHoliday': np.dtype(int),\n",
    "         'PromoInterval': np.dtype(str)}\n",
    "\n",
    "train = pd.read_csv(\"/media/EA34F85134F821ED/Python27/Kaggle/Rossmann/train_filled_gap.csv\", parse_dates=[2], dtype=types)\n",
    "test = pd.read_csv(\"/media/EA34F85134F821ED/Python27/Kaggle/Rossmann/test.csv\", parse_dates=[3], dtype=types)\n",
    "store = pd.read_csv(\"/media/EA34F85134F821ED/Python27/Kaggle/Rossmann/store.csv\")\n",
    "\n",
    "\n",
    "print(\"Assume store open, if not provided\")\n",
    "test.fillna(1, inplace=True)\n",
    "\n",
    "print(\"Consider only open stores for training. Closed stores wont count into the score.\")\n",
    "train = train[train[\"Open\"] != 0]\n",
    "print(\"Use only Sales bigger then zero\")\n",
    "train = train[train[\"Sales\"] > 0]\n",
    "\n",
    "print(\"Join with store\")\n",
    "train = pd.merge(train, store, on='Store')\n",
    "test = pd.merge(test, store, on='Store')\n",
    "\n",
    "features = []\n",
    "\n",
    "print(\"augment features\")\n",
    "train = build_features(features, train)\n",
    "test = build_features([], test)\n",
    "print(features)\n",
    "\n",
    "print('training data processed')\n",
    "\n",
    "def rmspe(y, yhat):\n",
    "    return np.sqrt(np.mean(((y - yhat)/y) ** 2))\n",
    "\n",
    "def rmspe_xg(yhat, y):\n",
    "    y = np.expm1(y.get_label())\n",
    "    yhat = np.expm1(yhat)\n",
    "    return \"rmspe\", rmspe(y, yhat)\n",
    "\n",
    "print(\"Train xgboost model\")\n",
    "\n",
    "params = {\"objective\": \"reg:linear\",\n",
    "          \"booster\" : \"gbtree\",\n",
    "          \"eta\": 0.1,\n",
    "          \"max_depth\": 10,\n",
    "          \"subsample\": 0.85,\n",
    "          \"colsample_bytree\": 0.4,\n",
    "          \"min_child_weight\": 6,\n",
    "          \"silent\": 1,\n",
    "          \"thread\": 1,\n",
    "          \"seed\": 1301\n",
    "          }\n",
    "num_boost_round = 1\n",
    "\n",
    "print(\"Train a XGBoost model\")\n",
    "X_train, X_valid = train_test_split(train, test_size=0.012, random_state=10)\n",
    "y_train = np.log1p(X_train.Sales)\n",
    "y_valid = np.log1p(X_valid.Sales)\n",
    "dtrain = xgb.DMatrix(X_train[features], y_train)\n",
    "dvalid = xgb.DMatrix(X_valid[features], y_valid)\n",
    "\n",
    "watchlist = [(dtrain, 'train'), (dvalid, 'eval')]\n",
    "gbm = xgb.train(params, dtrain, num_boost_round, evals=watchlist, early_stopping_rounds=200, \\\n",
    "  feval=rmspe_xg, verbose_eval=True)\n",
    "\n",
    "print(\"Validating\")\n",
    "yhat = gbm.predict(xgb.DMatrix(X_valid[features]))\n",
    "error = rmspe(X_valid.Sales.values, np.expm1(yhat))\n",
    "print('RMSPE: {:.6f}'.format(error))\n",
    "\n",
    "print(\"Make predictions on the test set\")\n",
    "dtest = xgb.DMatrix(test[features])\n",
    "test_probs = gbm.predict(dtest)\n",
    "# Make Submission\n",
    "result = pd.DataFrame({\"Id\": test[\"Id\"], 'Sales': np.expm1(test_probs)})\n",
    "result.to_csv(\"xgboost_39_submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function to splite Data & build DL,GBM,DRF,GLM to predict 'Customers'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ----------\n",
    "# 3- Fit a model on train; using test as validation\n",
    "\n",
    "# Function for doing class test/train/holdout split\n",
    "def split_fit_predict(data):\n",
    "    global gbm_cust,drf_cust,glm_cust,dl_cust\n",
    "    # Classic Test/Train split\n",
    "    r = data.runif()   # Random UNIForm numbers, one per row\n",
    "    train = data[r  <= r]\n",
    "    test  = data[(0.6 <= r) & (r < 0.9)]\n",
    "    hold  = data[ 0.9 <= r ]\n",
    "    print \"Training data has\",train.ncol,\"columns and\",train.nrow,\"rows, test has\",test.nrow,\"rows, holdout has\",hold.nrow\n",
    "    x = data.names\n",
    "    x.remove(\"Sales\")\n",
    "    x.remove('Customers')\n",
    "\n",
    "    # Run GBM\n",
    "    s = time.time()\n",
    "\n",
    "    gbm_cust = H2OGradientBoostingEstimator(ntrees=500, # 500 works well\n",
    "                                        max_depth=6,\n",
    "                                        learn_rate=0.1)\n",
    "\n",
    "\n",
    "    gbm_cust.train(x=x,\n",
    "               y=\"Customers\",\n",
    "               training_frame =train,\n",
    "               validation_frame=test)\n",
    "\n",
    "    gbm_elapsed = time.time() - s\n",
    "\n",
    "    # Run DRF\n",
    "    s = time.time()\n",
    "\n",
    "    drf_cust = H2ORandomForestEstimator(ntrees=250, max_depth=30)\n",
    "\n",
    "    drf_cust.train(x=x,\n",
    "               y=\"Customers\",\n",
    "               training_frame =train,\n",
    "               validation_frame=test)\n",
    "\n",
    "    drf_elapsed = time.time() - s \n",
    "\n",
    "\n",
    "    # Run GLM\n",
    "    # if \"WC1\" in bike_names_x: bike_names_x.remove(\"WC1\")\n",
    "    s = time.time()\n",
    "\n",
    "    glm_cust = H2OGeneralizedLinearEstimator(Lambda=[1e-5], family=\"poisson\")\n",
    "\n",
    "    glm_cust.train(x=x,\n",
    "               y=\"Customers\",\n",
    "               training_frame =train,\n",
    "               validation_frame=test)\n",
    "\n",
    "    glm_elapsed = time.time() - s\n",
    "\n",
    "    # Run DL\n",
    "    s = time.time()\n",
    "\n",
    "    dl_cust = H2ODeepLearningEstimator(hidden=[50,50,50,50], epochs=50)\n",
    "\n",
    "    dl_cust.train(x=x,\n",
    "              y=\"Customers\",\n",
    "              training_frame=train,\n",
    "              validation_frame=test)\n",
    "\n",
    "    dl_elapsed = time.time() - s\n",
    "\n",
    "    # ----------\n",
    "    # 4- Score on holdout set & report\n",
    "    train_r2_gbm = gbm_cust.model_performance(train).r2()\n",
    "    test_r2_gbm  = gbm_cust.model_performance(test ).r2()\n",
    "    hold_r2_gbm  = gbm_cust.model_performance(hold ).r2()\n",
    "    print \"GBM R2 TRAIN=\",train_r2_gbm,\", R2 TEST=\",test_r2_gbm,\", R2 HOLDOUT=\",hold_r2_gbm\n",
    "\n",
    "    train_r2_drf = drf_cust.model_performance(train).r2()\n",
    "    test_r2_drf  = drf_cust.model_performance(test ).r2()\n",
    "    hold_r2_drf  = drf_cust.model_performance(hold ).r2()\n",
    "    print \"DRF R2 TRAIN=\",train_r2_drf,\", R2 TEST=\",test_r2_drf,\", R2 HOLDOUT=\",hold_r2_drf\n",
    "\n",
    "    train_r2_glm = glm_cust.model_performance(train).r2()\n",
    "    test_r2_glm  = glm_cust.model_performance(test ).r2()\n",
    "    hold_r2_glm  = glm_cust.model_performance(hold ).r2()\n",
    "    print \"GLM R2 TRAIN=\",train_r2_glm,\", R2 TEST=\",test_r2_glm,\", R2 HOLDOUT=\",hold_r2_glm\n",
    "\n",
    "    train_r2_dl = dl_cust.model_performance(train).r2()\n",
    "    test_r2_dl  = dl_cust.model_performance(test ).r2()\n",
    "    hold_r2_dl  = dl_cust.model_performance(hold ).r2()\n",
    "    print \" DL R2 TRAIN=\",train_r2_dl,\", R2 TEST=\",test_r2_dl,\", R2 HOLDOUT=\",hold_r2_dl\n",
    "\n",
    "    # make a pretty HTML table printout of the results\n",
    "\n",
    "    header = [\"Model\", \"R2 TRAIN\", \"R2 TEST\", \"R2 HOLDOUT\", 'Test MSE', \"Model Training Time (s)\"]\n",
    "    table  = [\n",
    "              [\"GBM\", train_r2_gbm, test_r2_gbm, round(hold_r2_gbm), round(gbm_cust.mse()), round(gbm_elapsed,3)],\n",
    "              [\"DRF\", train_r2_drf, test_r2_drf, round(hold_r2_drf), round(drf_cust.mse()), round(drf_elapsed,3)],\n",
    "              [\"GLM\", train_r2_glm, test_r2_glm, round(hold_r2_glm), round(glm_cust.mse()), round(glm_elapsed,3)],\n",
    "              [\"DL \", train_r2_dl,  test_r2_dl,  round(hold_r2_dl), round(dl_cust.mse()), round(dl_elapsed,3) ],\n",
    "              ]\n",
    "    h2o.H2ODisplay(table,header)\n",
    "    # --------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Split the data (into test & train), fit some models and predict on the holdout data\n",
    "\n",
    "data = h2o.import_file('/ya/mro/yax90/oemro/Kaggle/train_prep.csv')\n",
    "split_fit_predict(data)\n",
    "# Here we see an r^2 of 0.91 for GBM, and 0.71 for GLM.  This means given just\n",
    "# the station, the month, and the day-of-week we can predict 90% of the\n",
    "# variance of the bike-trip-starts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test = h2o.import_file('/ya/mro/yax90/oemro/Kaggle/test_prep.csv')\n",
    "testNoID = test.drop('Id')\n",
    "\n",
    "a = 1\n",
    "b = 3\n",
    "c = 0\n",
    "d = 1\n",
    "yhat_cust = []\n",
    "yhat_cust =(a * gbm_cust.predict(testNoID) + b * drf_cust.predict(testNoID) + \\\n",
    "           c * glm_cust.predict(testNoID) + d * dl_cust.predict(testNoID))/sum([a,b,c,d])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test = test.as_data_frame(use_pandas=True)\n",
    "yhat_cust = yhat_cust.as_data_frame(use_pandas=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_cust = pd.concat([test,yhat_cust],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_cust.columns=['Id', 'Store', 'DayOfWeek', 'Open', 'Promo', 'StateHoliday', 'SchoolHoliday', \n",
    "                   'StoreType', 'Assortment', 'CompetitionDistance', 'CompetitionOpenSinceMonth', \n",
    "                   'CompetitionOpenSinceYear', 'Promo2', 'Promo2SinceWeek', 'Promo2SinceYear', 'year', \n",
    "                   'month', 'day', 'Customers']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_cust.to_csv('/ya/mro/yax90/oemro/Kaggle/test_cust.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_cust.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function to splite Data & build DL,GBM,DRF,GLM to predict 'Sales'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ----------\n",
    "# 3- Fit a model on train; using test as validation\n",
    "\n",
    "# Function for doing class test/train/holdout split\n",
    "def split_fit_predict(data):\n",
    "    global gbm_sales,drf_sales,glm_sales,dl_sales\n",
    "    # Classic Test/Train split\n",
    "    r = data.runif()   # Random UNIForm numbers, one per row\n",
    "    train = data[r  <= r]\n",
    "    test  = data[(0.6 <= r) & (r < 0.9)]\n",
    "    hold  = data[ 0.9 <= r ]\n",
    "    print \"Training data has\",train.ncol,\"columns and\",train.nrow,\"rows, test has\",test.nrow,\"rows, holdout has\",hold.nrow\n",
    "    x = data.names\n",
    "    x.remove('Customers')\n",
    "    x.remove(\"Sales\")\n",
    "\n",
    "    # Run GBM\n",
    "    s = time.time()\n",
    "\n",
    "    gbm_sales = H2OGradientBoostingEstimator(ntrees=500, # 500 works well\n",
    "                                        max_depth=6,\n",
    "                                        learn_rate=0.1)\n",
    "\n",
    "\n",
    "    gbm_sales.train(x=x,\n",
    "               y=\"Sales\",\n",
    "               training_frame =train,\n",
    "               validation_frame=test)\n",
    "\n",
    "    gbm_elapsed = time.time() - s\n",
    "\n",
    "    # Run DRF\n",
    "    s = time.time()\n",
    "\n",
    "    drf_sales = H2ORandomForestEstimator(ntrees=500, max_depth=50)\n",
    "\n",
    "    drf_sales.train(x=x,\n",
    "                    y=\"Sales\",\n",
    "                    training_frame =train,\n",
    "                    validation_frame=test)\n",
    "\n",
    "    drf_elapsed = time.time() - s \n",
    "\n",
    "\n",
    "    # Run GLM\n",
    "    # if \"WC1\" in bike_names_x: bike_names_x.remove(\"WC1\")\n",
    "    s = time.time()\n",
    "\n",
    "    glm_sales = H2OGeneralizedLinearEstimator(Lambda=[1e-5], family=\"poisson\")\n",
    "\n",
    "    glm_sales.train(x=x,\n",
    "               y=\"Sales\",\n",
    "               training_frame =train,\n",
    "               validation_frame=test)\n",
    "\n",
    "    glm_elapsed = time.time() \n",
    "    \n",
    "    # Run DL\n",
    "    s = time.time()\n",
    "\n",
    "    dl_sales = H2ODeepLearningEstimator(hidden=[50,50,50,50], epochs=50)\n",
    "\n",
    "    dl_sales.train(x=x,\n",
    "              y=\"Sales\",\n",
    "              training_frame=train,\n",
    "              validation_frame=test)\n",
    "\n",
    "    dl_elapsed = time.time() - s\n",
    "\n",
    "    # ----------\n",
    "    # 4- Score on holdout set & report\n",
    "    train_r2_gbm = gbm_sales.model_performance(train).r2()\n",
    "    test_r2_gbm  = gbm_sales.model_performance(test ).r2()\n",
    "    hold_r2_gbm  = gbm_sales.model_performance(hold ).r2()\n",
    "    print \"GBM R2 TRAIN=\",train_r2_gbm,\", R2 TEST=\",test_r2_gbm,\", R2 HOLDOUT=\",hold_r2_gbm\n",
    "\n",
    "    train_r2_drf = drf_sales.model_performance(train).r2()\n",
    "    test_r2_drf  = drf_sales.model_performance(test ).r2()\n",
    "    hold_r2_drf  = drf_sales.model_performance(hold ).r2()\n",
    "    print \"DRF R2 TRAIN=\",train_r2_drf,\", R2 TEST=\",test_r2_drf,\", R2 HOLDOUT=\",hold_r2_drf\n",
    "\n",
    "    train_r2_glm = glm_sales.model_performance(train).r2()\n",
    "    test_r2_glm  = glm_sales.model_performance(test ).r2()\n",
    "    hold_r2_glm  = glm_sales.model_performance(hold ).r2()\n",
    "    print \"GLM R2 TRAIN=\",train_r2_glm,\", R2 TEST=\",test_r2_glm,\", R2 HOLDOUT=\",hold_r2_glm\n",
    "\n",
    "    train_r2_dl = dl_sales.model_performance(train).r2()\n",
    "    test_r2_dl  = dl_sales.model_performance(test ).r2()\n",
    "    hold_r2_dl  = dl_sales.model_performance(hold ).r2()\n",
    "    print \" DL R2 TRAIN=\",train_r2_dl,\", R2 TEST=\",test_r2_dl,\", R2 HOLDOUT=\",hold_r2_dl\n",
    "\n",
    "    # make a pretty HTML table printout of the results\n",
    "\n",
    "    header = [\"Model\", \"R2 TRAIN\", \"R2 TEST\", \"R2 HOLDOUT\", 'Test MSE', \"Model Training Time (s)\"]\n",
    "    table  = [\n",
    "              [\"GBM\", train_r2_gbm, test_r2_gbm, round(hold_r2_gbm), gbm_sales.mse(), round(gbm_elapsed,3)],\n",
    "              [\"DRF\", train_r2_drf, test_r2_drf, round(hold_r2_drf), drf_sales.mse(), round(drf_elapsed,3)],\n",
    "              [\"GLM\", train_r2_glm, test_r2_glm, round(hold_r2_glm), glm_sales.mse(), round(glm_elapsed,3)],\n",
    "              [\"DL \", train_r2_dl,  test_r2_dl,  round(hold_r2_dl), dl_sales.mse(), round(dl_elapsed,3) ],\n",
    "              ]\n",
    "    h2o.H2ODisplay(table,header)\n",
    "    # --------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = h2o.import_file('/media/EA34F85134F821ED/Python27/train_prep.csv')\n",
    "split_fit_predict(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test = h2o.import_file('/ya/mro/yax90/oemro/Kaggle/test_cust.csv')\n",
    "testNoID = test.drop('Id')\n",
    "\n",
    "a = 2\n",
    "b = 2\n",
    "c = 0\n",
    "d = 1\n",
    "yhat_sales = []\n",
    "yhat_sales =(a * gbm_sales.predict(testNoID) + b * drf_sales.predict(testNoID) + \\\n",
    "           c * glm_sales.predict(testNoID) + d * dl_sales.predict(testNoID))/sum([a,b,c,d])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test = test.as_data_frame(use_pandas=True)\n",
    "yhat_sales = yhat_sales.as_data_frame(use_pandas=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_sales = pd.concat([test,yhat_sales],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_sales.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_sales.columns=['Id', 'Store', 'DayOfWeek', 'Open', 'Promo', 'StateHoliday', 'SchoolHoliday', \n",
    "                   'StoreType', 'Assortment', 'CompetitionDistance', 'CompetitionOpenSinceMonth', \n",
    "                   'CompetitionOpenSinceYear', 'Promo2', 'Promo2SinceWeek', 'Promo2SinceYear', 'year', \n",
    "                   'month', 'day', 'Customers','Sales']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_sales.to_csv('/ya/mro/yax90/oemro/Kaggle/test_sales.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_sales['Sales'] = np.expm1(test_sales['Sales'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_sales[['Id','Sales']].sort(columns='Id').to_csv('/ya/mro/yax90/oemro/Kaggle/submission.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fit best Params to DRF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def split_fit_predict_drf(data):\n",
    "    global drf_sales\n",
    "    # Classic Test/Train split\n",
    "    r = data.runif()   # Random UNIForm numbers, one per row\n",
    "    train = data[r  <= r]\n",
    "    test  = data[(0.6 <= r) & (r < 0.9)]\n",
    "    hold  = data[ 0.9 <= r ]\n",
    "    print \"Training data has\",train.ncol,\"columns and\",train.nrow,\"rows, test has\",test.nrow,\"rows, holdout has\",hold.nrow\n",
    "    x = data.names\n",
    "    x.remove('Customers')\n",
    "    x.remove(\"Sales\")\n",
    "\n",
    "    # Run DRF\n",
    "    s = time.time()\n",
    "\n",
    "    drf_sales = H2ORandomForestEstimator(ntrees=500, max_depth=50,balance_classes=True, nbins_cats=1115)\n",
    "\n",
    "    drf_sales.train(x=x,\n",
    "                    y=\"Sales\",\n",
    "                    training_frame =train,\n",
    "                    validation_frame=test)\n",
    "\n",
    "    drf_elapsed = time.time() - s "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data = h2o.import_file('/media/EA34F85134F821ED/Python27/Kaggle/train_prep.csv')\n",
    "split_fit_predict_drf(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make Prediction, Transform H2Odf to PDdf, Write Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test = h2o.import_file('/media/EA34F85134F821ED/Python27/Kaggle/test_prep.csv')\n",
    "testNoID = test.drop('Id')\n",
    "\n",
    "#gbm_predict = gbm_sales.predict(testNoID)\n",
    "drf_predict = drf_sales.predict(testNoID)\n",
    "#glm_predict = glm_sales.predict(testNoID)\n",
    "#dl_predict = dl_sales.predict(testNoID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test = test.as_data_frame(use_pandas=True)\n",
    "#gbm_predict = gbm_predict.as_data_frame(use_pandas=True)\n",
    "drf_predict = drf_predict.as_data_frame(use_pandas=True)\n",
    "#glm_predict = glm_predict.as_data_frame(use_pandas=True)\n",
    "#dl_predict = dl_predict.as_data_frame(use_pandas=True)\n",
    "#yhat_sales = yhat_sales.as_data_frame(use_pandas=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ensemble25 = pd.read_csv('D:\\Python27\\Kaggle\\Rossmann/prediction_25_DL_models_avg.csv', index_col=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ensemble25.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ensemble25['Avg'] = ensemble25['Avg']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ensemble25.Avg.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.expm1(ensemble25['Avg'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test = pd.read_csv('D:\\Python27\\Kaggle\\Rossmann/test_prep.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "avg = np.expm1(ensemble25['Avg'])\n",
    "avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ensemble25.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ensemble25['Avg'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "drf = pd.concat([test['Id'],ensemble25['Sales']],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "drf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "drf.sort(columns='Id').to_csv('D:\\Python27\\Kaggle\\Rossmann/ensemble_25.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#test_sales = pd.concat([test['Id'],yhat_sales],axis=1)\n",
    "#gbm = pd.concat([test['Id'],gbm_predict],axis=1)\n",
    "drf = pd.concat([test['Id'],drf_predict],axis=1)\n",
    "#glm = pd.concat([test['Id'],glm_predict],axis=1)\n",
    "#dl = pd.concat([test['Id'],dl_predict],axis=1)\n",
    "\n",
    "#test_sales.columns=['Id', 'Sales']\n",
    "#gbm.columns=['Id', 'Sales']\n",
    "drf.columns=['Id', 'Sales']\n",
    "#glm.columns=['Id', 'Sales']\n",
    "#dl.columns=['Id', 'Sales']\n",
    "\n",
    "#test_sales['Sales'] = np.expm1(test_sales['Sales'])\n",
    "#gbm['Sales'] = np.expm1(gbm['Sales'])\n",
    "drf['Sales'] = np.expm1(drf['Sales'])\n",
    "#glm['Sales'] = np.expm1(glm['Sales'])\n",
    "#dl['Sales'] = np.expm1(dl['Sales'])\n",
    "\n",
    "#test_sales.sort(columns='Id').to_csv('/ya/mro/yax90/oemro/Kaggle/submission.csv',index=False)\n",
    "#gbm.sort(columns='Id').to_csv('/media/EA34F85134F821ED/Python27/Kaggle/submission_gbm.csv',index=False)\n",
    "drf.sort(columns='Id').to_csv('/media/EA34F85134F821ED/Python27/Kaggle/submission_drf.csv',index=False)\n",
    "#glm.sort(columns='Id').to_csv('/media/EA34F85134F821ED/Python27/Kaggle/submission_glm.csv',index=False)\n",
    "#dl.sort(columns='Id').to_csv('/media/EA34F85134F821ED/Python27/Kaggle/submission_dl.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Root Mean Square Prozent Error from func rmspe "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rmspe(train.Sales,drf_predict.predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = h2o.import_file('/media/EA34F85134F821ED/Python27/Kaggle/train_prep.csv')\n",
    "r = data.runif()\n",
    "train = data[r  <= r]\n",
    "test  = data[(0.6 <= r) & (r < 0.9)]\n",
    "hold  = data[ 0.9 <= r ]\n",
    "print \"Training data has\",train.ncol,\"columns and\",train.nrow,\"rows, test has\",test.nrow,\"rows, holdout has\",hold.nrow\n",
    "x = data.names\n",
    "x.remove('Customers')\n",
    "x.remove(\"Sales\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "drf_predict = drf_sales.predict(train)\n",
    "drf_predict = drf_predict.as_data_frame(use_pandas=True)\n",
    "train = train.as_data_frame(use_pandas=True)\n",
    "\n",
    "rmspe(train.Sales,drf_predict.predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grid Search DRF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from h2o.grid.grid_search import H2OGridSearch\n",
    "\n",
    "data = h2o.import_file('/media/EA34F85134F821ED/Python27/Kaggle/train_prep.csv')\n",
    "\n",
    "# Extract best Model from Grid Search\n",
    "def extractBestModel(models):\n",
    "    bestMse = models[0].mse(xval=True)\n",
    "    result = models[0]\n",
    "    for model in models:\n",
    "        if model.mse(xval=True) < bestMse:\n",
    "            bestMse = model.mse(xval=True)\n",
    "            result = model\n",
    "    return result\n",
    "\n",
    "\n",
    "# Classic Test/Train split\n",
    "r = data.runif()   # Random UNIForm numbers, one per row\n",
    "train = data[r  <= 0.2]\n",
    "test  = data[(0.6 <= r) & (r < 0.9)]\n",
    "hold  = data[ 0.9 <= r ]\n",
    "print \"Training data has\",train.ncol,\"columns and\",train.nrow,\"rows, test has\",test.nrow,\"rows, holdout has\",hold.nrow\n",
    "x = data.names\n",
    "x.remove(\"Sales\")\n",
    "x.remove('Customers')\n",
    "\n",
    "# Run DRF\n",
    "hyper_parameters = {'ntrees':[10,500], 'max_depth':[100,50,10]}\n",
    "#,'balance_classes':True, 'nbins_cats':1115}\n",
    "grid_search = H2OGridSearch(H2ORandomForestEstimator, hyper_params=hyper_parameters)\n",
    "grid_search.train(x=x, y=\"Sales\", training_frame=train)\n",
    "\n",
    "                              \n",
    "bestmodel = extractBestModel(grid_search)\n",
    "bestmodel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bestmodel3 = grid_search[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bestmodel3.get_params"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
